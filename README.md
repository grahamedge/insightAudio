# Insight Audio Analysis Project

This is a project which broadly aimed to identify different speakers in an audio file - a task known as 'speaker diarization'. Specifically we wanted to analyse the audio data from teacher demonstration videos that are posted to Youtube, to see if we can detect the differences between 'teacher talking' and 'teacher not talking'. The latter case might correspond to students talking, or might equally well correspond to long periods of silence.I performed this project in consultation with a start-up company who wants to improve the way that teachers are hired in school districts. Having knowledge of 'who is talking' in a demonstration video allows for automatic extraction of metrics like the "Teacher Talk Ratio" which are useful to school districts who want measurable traits with which to sort and filter teaching applicants. 

Since I had no labelled data with which to train a model, I relied on unsupervised learning techniques for this analysis. I used an approach which divided each audio file into half-second long time bins, and then generated a small number of audio features which represent the pitch and timbre present at that time. Agglomerative clustering techniques are used on these audio features to discover the presence of different voices in the audio file. If two distinct clusters can be discovered in the audio feature space, then each second of the audio file can be labelled as either 'Speaker A' or 'Speaker B' depending the cluster to which it belongs.  The specific audio features that worked best for this task were the [Mel Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum).

The process followed for the project is:

1. Download youtube videos: each video is specified by its unique Youtube ID such as 'dQw4w9WgXcQ' for the video 'https://www.youtube.com/watch?v=dQw4w9WgXcQ', and the audio files are downloaded and processed using functions in the 'Youtube' directory

2. Convert the audio waveform into useable information: functions in the directory 'Audio' are used to extract meaning from the sound wave using fourier analysis, the Mel Frequency Cepstrum, and cluster analysis

3. Save the audio analysis results in a database: functions in the directory 'SQL' handle the transfer of information to and from a PostgreSQL database hosted on Amazon RDS

4. View the analysis results in a web interface: using Flask a simple web interface has been created to view the audio analysis results along with the original video files, based on the functions contained in the 'Flask_Frontend' directory

Unfortunately, the downloading and audio feature extraction are too slow to run in real time, so my web app only shows a selection of videos that I have previously downloaded and processed.

# Technical Details

## Teacher Demonstration Videos
To apply for teaching jobs, teachers typically need to submit a Teaching Demonstration Video showing them in a classroom interacting with students. The teachers know that they are being filmed, and the students do too! This is obviously a bit of a weird dynamic in the classroom, but it is still an excellent source of information for school boards that want to get a sense of the teaching style of a particular individual. There may be some broad qualitative traits that one would identify about a teacher while watching such a video, and by watching two or three such videos one might be able to instinctively identify the teacher who would be the ‘best fit’ for a school’s vacancy. However, the challenge is rarely to view just one or two videos. The stack of applications received for teaching positions in public schools can be immense and so the task of watching them all - and finding ways to rank them against one another – is immense. One approach to try to compare a large number of teachers against one another is to find reasonable quantitative metrics that are known to be correlated with or predictive of effective teaching

- the % of time the teacher is addressing the entire classroom
- the number of student-teacher interactions
- the number of different students with whom the teacher interacts
- the amount of time it takes students to quiet down in transition activities
- the volume at which the teacher speaks, and its variation
- the number of times a teacher raises his/her voice

## Extracting predictive features from video data
Most of the above features can be fairly well-defined mathematically. This makes it possible to distribute the task of ranking teacher videos among several different viewers, and be confident that the results of the ranking are not strongly subjective. The question at the beginning of this project was, “which of these teacher metrics could be automatically extracted from a video data file?” Some of these tasks appear to need the combination of several machine learning tools. For example, to determine how long it takes students to quiet down after being asked, one first needs to predict when students have been asked to quiet down. This command might be delivered forcefully or calmly by a teacher, and is more likely to be issued during a period in the classroom where a lot of voices and/or audio noise are present. This means that a strong audio signal for “teacher asks students to quiet down” is likely to be difficult to isolate. Other approached may be more fruitful; if transcripts of an teaching video are available they could be mined to locate verbal commands to the students. To keep the greatest hope of creating a working project within three-weeks, I elected to focus on calculating the teacher-talk-ratio. To extract the fraction of time that a teacher is speaking in a classroom video, one can attempt to classify each second of audio as “teacher” or “not-teacher”. To frame this as a data science problem, I needed to find some features that describe the sound in the classroom at any particular moment in the video, and then build a classification model that takes those features as inputs and returns a prediction for “teacher” or “not teacher”.

## Finding Features in Audio Data
Working directly on the raw audio waveform is a terrible idea. This is just a rapid measurement of the air pressure in the room, and all of the information of what sound is actually produced is captured in the structure of the air pressure over time. For this reason, any one measurement is practically useless, and one needs to look in a feature space that captures the relationships between different time points. From my background in physics I know a lot about the frequency representation of time-series data, and so I launched into an inspection of the audio files with several great ideas for how to engineer audio features. Unfortunately I was too focussed on the easy-to-define aspects of an audio signal and so all of my ideas turned out to be wrong. Audio is sometimes described as having three components: loudness, pitch, and timbre. Loudness is correlated with the overall amplitude or pressude of the sound wave, but there are physiological factors that come into play as well. Two equal amplitude signals at different audio frequencies will no be judged to have equal loudness by a human observer. The situation is similar for pitch. To create a strict definition one can rigorously associate the pitch of a pure tone with the fundamental frequency with which the air pressure oscillates. However the concept of human perception creeps in once again since the human ear becomes less sensitive to frequency differences as the audio frequency increases towards the higher end of human audio perception (20kHz). Timbre is the trickiest idea of all, since it is a catch-all term to describe all of the things that can be different about two sounds that have the same loudness and pitch. For instance, timbre is what allows you to distinguish a clarinet and a saxaphone which are both playing the note A4 at 440Hz. These two instruments will produce different overtones or harmonics of the main 440Hz oscillation, and one can think of timbre as capturing some of this structure that occurs in the frequency representation of the signal. Since a single person talking can hit a variety of loudness levels and pitches during a normal conversation, they might be difficult features to focus on when trying to identify a speaker in audio. It is not to say that these features can’t work - students in the class might generally have higher pitched voices than the teacher, or might be overall more quiet in the classroom. Based on a survey of other audio analysis work, I decided that I would try to look at the timbre of the audio by focussing on patterns and structure in the frequency representation of the audio.

## Structure in Frequency Space
A great way to visualize strucure in frequency space is by looking at an audio spectrogram. To generate a spectrogram, an audio signal is broken up into many short time bins, and then a fourier transform is calculated for each time bin to find its frequency spectrum. The frequency spectrum gives a weight for each possible frequency, and these weights are calculated for each time bin in the file. This information can be plotted by arranging all of the possible time bins and frequencies along the two axes of a plot, and using a colormap to label the frequency amplitudes at each point in the plot. This produces an image of the audio waveform, as shown below. Both of these images are lifted [this youtube video](https://www.youtube.com/watch?v=P9Kozlt0tTg) which is a fun watch if you have 3 minutes to spare.


![Saxaphone spectrogram](http://www.grahamedge.com/static/Saxaphone.png)
*Spectrogram produced by a saxaphone*

![Accordion spectrogram](http://www.grahamedge.com/static/Accordion.png)
*Spectrogram produced by an accordion*

There is almost an overwhelming amount of data captured in the spectrogram. With large sets of labelled data, a good approach might be to train a convolutional neural net to look at the spectrogram like an image, capturing information about different frequencies that appear at the same time as well as the frequencies that appear close together in time. This kind of approach can be used to identify spoken words, or to identify the language spoken in audio. Without a large set of labelled training data, I wouldn’t get very far in trying to train a neural network. Instead I wanted a more compact representation for the timbre information with which I could try to look for structure. The most common way to approach this in prior audio analysis work is to calculate the Mel Frequency Cepstral Coefficients.

## Why MFCCs?
The Mel Frequency Cepstral Coefficients (hereafter MFCCs) are common features for audio analysis. For instance, used in speaker verification, laughter detection, and musical instrument identification. They are the coefficients of the discrete cosine transform that operates on a modified frequency space representation of the signal. The modified frequency space is formed by collecting the frequency spectrum of a signal and applying a set of filterbanks that mimic the ranges of frequencies that humans can distinguish (using the Mel scale), and then taking the log of the frequency spectral amplitudes. Due to the way that the cosine transform is defined, the 0th order MFCC measures the total amplitude of the signal in the transformed frequency space. This means that the 0th order MFCC carries some information about the loudness of the sound. The other MFCCs measure certain kinds of periodicity in the modified frequency space, which is what allows them to capture some sense of the harmonic structure of a sound and thus its timbre. To explore whether the various features for loudness, pitch, or timbre would actually allow us to separate different voices, I hand-labelled a classroom video to identify the seconds in which the teacher was talking. Then I plotted all of the time bins in the audio file in a two-dimensional audio feature space to explore whether the different voices appeared in distinct regions of the plot. Since there were many audio features to deal with, I used principal component analysis (PCA) to identify two composite audio features that I could plot in a flat image.

![MFCC Features](http://www.grahamedge.com/static/MFCC_features.png)

*A view into the MFCC feature space, with Teacher = red and Students = black*

![MFCCs and Pitch Features](http://www.grahamedge.com/static/MFCC_and_Chroma.png)

*A view into a feature space of MFCCs as well as Pitch features*

The plots above show these two-dimensional images derived from different types of audio features. In all cases the time bins in which the teacher or students are speaking are colored red and black, respectively. In an ideal world, the audio features for a teacher's vs. a student's voice would be so different that they would occur in completely different regions of this two-dimensional image. In the first image, only the MFCC features have been handed over to the PCA algorithm. In the resulting plot the red and black points are slightly separated, but do not form distinct and unrelated clusters that can be identified by eye. In the second plot, the same MFCC features have been combined with additional features that correspond to pitch instead of timbre. These "chroma" features are very useful for musical analysis since they map well onto the twelve tones of a standard musical scale. However, the two-dimensional projection returned by PCA shows an increased overlap of the red and black points - the opposite of the speaker separation that we would like to achieve. After trying to visualize many of the possible audio features in this way, I concluded that only the MFCC features seemed to be different for the two types of voices. The next step of the problem was to apply some unsupervised learning technique to try to detect the different speakers based on their MFCC features, without having any labelling to identify red or black points.

## Clustering the MFCC Features
To create visualizations that can be easily viewed by a human, only two variables are desired. However, if the goal is to discover clusters of points in MFCC space, reducing to only two features is unnecessary. We could keep all twelve of the original MFCC features, or perhaps reduce to a smaller subset of more important features by again using PCA. In cases where I could run the model on a labelled video, I found that the performance was generally better if I reduced the feature space from twelve MFCCs to about five composite features. This reduction might help to avoid the "curse of dimensionality", or might just help to minimize the effect of certain MFCC features which are strongly correlated or which have little variation in the data.

![Unlabelled waveform](http://www.grahamedge.com/static/Unlabelled.png)

*Cartoon of an unlabelled waveform*

![Labelled waveform](http://www.grahamedge.com/static/Labelled.png)

*Waveform labelled with red and black for speaker A and B*

Once I had a reduced set of features, I could calculate the distances between all of the time bins of the audio file (where distance is measured in this reduced features space) and then use hierarchical clustering to find the groups of time bins which are most similar to one another. Depending on the assignment of each time bin into the two largest clusters, I could label each the entire audio waveform as “Speaker A” or “Speaker B”. This is shown above, where a cartoon audio waveform is color coded into black and red segments. It is important to note that due to the unsupervised nature of the model, I can’t actually say which speaker is the teacher! An extension to the project could try to find a way to determine which of many clusters corresponds to a teacher by training an additional classifier!

## Clustering the Seconds in a Time Series
One idea that nagged at me while I was performing the clustering analysis was the complete ignorance of the temporal information. Since people in a conversation typically tend to speak for a few seconds at a time, any one second in the audio file is fairly likely to be spoken by the same person who spoke the previous few seconds. Looking at each second of audio as an independant point in “audio feature space” throws this useful information away. The problem isn’t just a theoretical one – the classification of the audio signal can jump back and forth between one speaker and the other erratically at some points in the audio file, in a way that clearly doesn’t correspond to the patterns of normal human speech. I considered several approaches to try to bring the temporal information back into the model:

1. Use a few-second-long time bin to group the data together, and calculate only average audio features inside this large bin. This would weight several consecutive seconds together when generating the audio features. However, a student who speaks for a time that is short compared to the audio time bin would be ignored by the algorithm, since their distinct audio features would be overwhelmed by the several other seconds in the audio.
2. Apply a median filter to the audio signal after the clustering analysis has been appied. This would force the label for each second to be more like its neighbors. I wasn’t excited about this approach, because a single time bin that really truly sounded very different from its neighboring time points would be treated exactly the same as a time point for which the audio features were right on the edge of being in one class or another. Since some student questions are short enough to exist in only one or two time bins in my analysis, median filtering might again overwhelm the student portions of the audio by the dominant teacher parts of the audio.
3. Expand my feature set, so that for each time bin of audio I consider the MFCCs calculated on that time bin, but also the MFCCs calculated from the two time bins that came before and the two that come after. I reasoned that this would have the effect of making each time point look similar to its neighboring time points, and this similarity would be encoded before the time binned features get separated and fed into the clustering algorithm as independant measurements.
4. I could define a new distance metric to use when judging which data points are “closest to one another” in the clustering algorithm. It’s not hard to define a distance metric that would make sense, but writing one in Python would slow down the clustering algorithm substantially (since the distance calculation is currently handled by scikit-learn, which has fast C code to do all of the heavy lifting).

I didn’t have enough time in the end to implement Option 4, so I used Option 3 to run all of my analysis. This performed a bit better than median filtering to remove the erratic jumps in the classification output over time, even though I don’t feel that it is as beautiful of an idea as a custom distance metric.

## Results
There are some situations where the clusters output by the unsupervised algorithm correspond quite well to two different speakers. For example, watch what happens when the model is applied to an interview of Frank Sinatra by his daughter Nancy Sinatra. Although there are short periods in which the classification fails to capture a quick question by Nancy, overall the agreement is fairly good. I tweaked the hyperparameters of the unsupervised learning algorithm while monitoring thea accuracy with which it classified the seconds of a hand-labelled training video. I was rather pleased that after some parameter optimization, the model could identify 80% of the time bins in the video correctly. This lead to predictions of the "Teacher Talk Ratio" and "Number of Student Interactions" that seemed very reasonable compared to the true values I had measured by watching the video.

## Drawbacks
However, when I pushed the model to generalize to a sample of other classroom videos, the performance was much worse. It seemed that there were certain edge cases, particular to each classroom video, that would throw the model for a loop. In some classes, a long period of silence would be detected as the most significant cluster in feature space, and all of the voices - both teacher and student - would be lumped together into the other cluster by the algorithm. In other cases, periods of music would be detected as a significantly different "voice" in the audio, and the model would again lump together the teacher and student voices in one large cluster. These were indications that the method used was too general. I was separating the two most different parts of the audio, regardless of whether this was Teacher vs. Student voices, Voical Sounds vs. Music, or Noise vs. Silence. To make the unsupervised approach very robust would require and identification of the various kinds of significant audio events that can occur in a classroom, followed by the development of safeguards to help the algorithm correct for each event. To keep the cluster analysis from latching on to a brief but distinct patch of audio, I added a check to the model output to detect strangely small clusters and re-merge them with the rest of the audio file. Further fixes of this sort could help to work around periods of silence in the file, but I had no further time to implement such fixes within the three-week time frame of Insight. Trying to take the unsupervised clustering approach and make it more robust to classroom noise would be an exciting extension of this work. Alternatively, adding a small set of labelled data could help to improve the classification accuracy by attempting to train on the different audio structure of adults’ and childrens’ voices. 
