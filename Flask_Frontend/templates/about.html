<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>TeacherReel</title>

    <!-- Bootstrap core CSS -->
    <link href="../static/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="../../assets/js/ie-emulation-modes-warning.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    </head>
    <body>


  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="http://www.grahamedge.com">TeacherReel</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="http://www.grahamedge.com">Home</a></li>
            <li class='active'><a href='http://www.grahamedge.com/about'>About the Project</a></li>
            <li class='active'><a href='http://www.grahamedge.com/me'>About Me</a></li>

            <li><a href="http://www.grahamedge.com/contact">Contact</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="jumbotron">
      <div class="container">
        <div class="col-md-4">
        <img src='/static/Graham_Edge_Square.jpg' class='img-responsive' alt="Hi there!">
        </div>
        <div class="col-md-6">
        <h1>Graham Edge</h1>
        <p>Data Scientist</p>
        </div>
      </div>
    </div>



<div class="container">

    <h2>About the Project</h2>

    <div id="accordion" role="tablist" aria-multiselectable="true">
  <div class="card">
    <div class="card-header" role="tab" id="headingOne">
      <h5 class="mb-0">
        <a class="btn btn-primary btn-lg" data-toggle="collapse" data-parent="#accordion" href="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
          The Elevator Pitch
        </a>
      </h5>
    </div>

    <div id="collapseOne" class="collapse" role="tabpanel" aria-labelledby="headingOne">
      <div class="card-block">
        As a fellow in the Insight Data Science program, I completed a project in three weeks in order to learn new data science skills and to showcase my capabilities. I worked with Youtube videos of in-class teacher demonstrations to develop unsupervised learning models which could identify whether the teacher or students were speaking at any given moment. This information is useful to hiring managers in public schools, who hae little time to personally review the demonstration videos which are submitted by each candidate, but who would like to filter out teachers whom they feel talk too much or too little.

        When analyzing these classroom videos, I was really excited to learn that there are good low-dimensional feature spaces that can encode the timbre of sound!
      </div>
    </div>
  </div>
  <div class="card">
    <div class="card-header" role="tab" id="headingTwo">
      <h5 class="mb-0">
        <a class="btn btn-primary btn-lg collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
          The Brief Version
        </a>
      </h5>
    </div>
    <div id="collapseTwo" class="collapse" role="tabpanel" aria-labelledby="headingTwo">
      <div class="card-block">
        <p>I am currently a fellow at Insight Data Science in NYC. All of the fellows in the program take part in a 3 week sprint to take a project from beginning to completion. My project tackled the challenge of speaker diarization in a noisy environment (speaker diarization is the task of determining who is speaking in recorded audio). I performed this project in consultation with a start-up company who wanted to find out the amount of time a teacher spends talking during a classroom demonstration video. This type of information is useful to school districts who want measurable traits with which to sort and filter teaching applicants.</p>

        <p>Since I had no labelled data with which to train a model, I relied on unsupervised learning techniques for this analysis. I used an approach which divided each audio file into half-second long time bins, and then generated a small number of audio features which represent the pitch and timbre present at that time. I used agglomerative clustering techniques in this audio feature space to try to discover the presence of different voices in the audio file. If two distinct clusters can be discovered in the audio feature space, then each second of the audio file can be labelled as either 'Speaker A' or 'Speaker B' depending the cluster to which it belongs.</p>

        <div class='row'>
          <div class="col-6 col-md-6">
            <img class="img-responsive img-centred" src="/static/Unlabelled.png" />
            <h4 style="text-align:center">Unlabelled waveform</h4>
          </div>
        </div>
        <div class='row'>
          <div class="col-6 col-md-6">
            <img class="img-responsive img-centred" src="/static/Labelled.png" />
            <h4 style="text-align:center">Labelled waveform</h4>
          </div>
        </div> 

        <p>In the case of teacher classroom videos, the clustering analysis tends to separate the audio file into times that can be labelled as 'Teacher Talking' and time that can be labelled as 'Everything Else'. It is not important that each individual student who might talk during the class be recognized as a distinct voice. The main interest fr this proof-of-concept project was to determine the fraction of time that the teacher spends talking - to possibly identify teachers who talk too much or too little. The actual notion of how much talking would be "too much" or "too little" would be drawn from educational literature.</p>

        <p>In the three weeks that I worked on this project, I had some success getting the unsupervised model to identify the speakers in the classroom audio to a better degree than one would expect with random guessing. However, the performance was not excellent, and it seems that this approach to analysis can be limited by the large variety of events which can occur in the noisy environment of an elementary school classroom. For instance, the clustering algorithm must needs to be checked to ensure that it does not produce clusters corresponding to a single loud and noisy event (like a phone ringing), and similar care must be taken to keep the clustering algorithm from attempting to group together all of the silence in the audio file. It isn't surprising that an unsupervised approach with unlabelled data struggles to pick up the details of such messy audio data, and to move forward with this project it may be to develop a training set of labelled data. To help structure the further work necessary, I am providing my start-up company a list of edge cases and common errors made by the model.</p>

        <p>In hopes of determining that the general machine learning approach was still valid, I tried to apply the same techniques to a variety of simpler audio files, to try to isolate the particular features of classroom audio that cause the most trouble. A good example of an easier audio file is a recorded interview between two individuals of opposite gender. As you can see in <a href='/video_output?yt_id=dqPjgQwoXLQ'>this example</a> the classification produced by the model under these circumstances is rather good!</p>

        <p>To see some more technical details of the project, why not try the longer version below, or perhaps poke around my <a href='https://github.com/grahamedge/insight_audio_analysis'>GitHub repo</a>? </p>
      </div>
    </div>
  </div>
  <div class="card">
    <div class="card-header" role="tab" id="headingThree">
      <h5 class="mb-0">
        <a class="btn btn-primary btn-lg collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="false" aria-controls="collapseThree">
          The Long Story
        </a>
      </h5>
    </div>
    <div id="collapseThree" class="collapse" role="tabpanel" aria-labelledby="headingThree">
      <div class="card-block">
        <h3>Teacher Demonstration Videos</h3>

        To apply for teaching jobs, teachers typically need to submit a Teaching Demonstration Video showing them in a classroom interacting with students.  The teachers know that they are being filmed, and the students do too! This is obviously a bit of a weird dynamic in the classroom, but it is still an excellent source of information for school boards that want to get a sense of the teaching style of a particular individual. There may be some broad qualitative traits that one would identify about a teacher while watching such a video, and by watching two or three such videos one might be able to instinctively identify the teacher who would be the ‘best fit’ for a school’s vacancy.

        However, the challenge is rarely to view just one or two videos. The stack of applications received for teaching positions in public schools can be immense and so the task of watching them all - and finding ways to rank them against one another – is immense. One approach to try to compare a large number of teachers against one another is to find reasonable quantitative metrics that are known to be correlated with or predictive of effective teaching 
        <!-- [refs] -->

        <ul>
        <li> the % of time the teacher is addressing the entire classroom</li>
        <li> the number of student-teacher interactions</li>
        <li> the number of different students with whom the teacher interacts</li>
        <li> the amount of time it takes students to quiet down in transition activities</li>
        <li> the volume at which the teacher speaks, and its variation</li>
        <li> the number of times a teacher raises his/her voice</li>
        </ul>

        <h3>Extracting predictive features from video data</h3>

        Most of the above features can be fairly well-defined mathematically. This makes it possible to distribute the task of ranking teacher videos among several different viewers, and be confident that the results of the ranking are not strongly subjective. The question at the beginning of this project was, “which of these teacher metrics could be automatically extracted from a video data file?”  

        Some of these tasks appear to need the combination of several machine learning tools. For example, to determine how long it takes students to quiet down after being asked, one first needs to predict when students have been asked to quiet down. This command might be delivered forcefully or calmly by a teacher, and is more likely to be issued during a period in the classroom where a lot of voices and/or audio noise are present. This means that a strong audio signal for “teacher asks students to quiet down” is likely to be difficult to isolate. Other approached may be more fruitful; if transcripts of an teaching video are available they could be mined to locate verbal commands to the students.

        To keep the greatest hope of creating a working project within three-weeks, I elected to focus on calculating the teacher-talk-ratio. To extract the fraction of time that a teacher is speaking in a classroom video, one can attempt to classify each second of audio as “teacher” or “not-teacher”.  To frame this as a data science problem, I needed to find some features that describe the sound in the classroom at any particular moment in the video, and then build a classification model that takes those features as inputs and returns a prediction for “teacher” or “not teacher”.

        <h3>Finding Features in Audio Data</h3>

        Working directly on the raw audio waveform is a terrible idea. This is just a rapid measurement of the air pressure in the room, and all of the information of what sound is actually produced is captured in the structure of the air pressure over time. For this reason, any one measurement is practically useless, and one needs to look in a feature space that captures the relationships between different time points. From my background in physics I know a lot about the frequency representation of time-series data, and so I launched into an inspection of the audio files with several great ideas for how to engineer audio features.  Unfortunately I was too focussed on the easy-to-define aspects of an audio signal and so all of my ideas turned out to be wrong.

        Audio is sometimes described as having three components: loudness, pitch, and timbre. Loudness is correlated with the overall amplitude or pressude of the sound wave, but there are physiological factors that come into play as well. Two equal amplitude signals at different audio frequencies will no be judged to have equal loudness by a human observer. The situation is similar for pitch. To create a strict definition one can rigorously associate the pitch of a pure tone with the fundamental frequency with which the air pressure oscillates. However the concept of human perception creeps in once again since the human ear becomes less sensitive to frequency differences as the audio frequency increases towards the higher end of human audio perception (20kHz).

        <!-- [overtones?] -->

        Timbre is the trickiest idea of all, since it is a catch-all term to describe all of the things that can be different about two sounds that have the same loudness and pitch. For instance, timbre is what allows you to distinguish a clarinet and a saxaphone which are both playing the note A4 at 440Hz.  These two instruments will produce different overtones or harmonics of the main 440Hz oscillation, and one can think of timbre as capturing some of this structure that occurs in the frequency representation of the signal.

        Since a single person talking can hit a variety of loudness levels and pitches during a normal conversation, they might be difficult features to focus on when trying to identify a speaker in audio. It is not to say that these features can’t work - students in the class might generally have higher pitched voices than the teacher, or might be overall more quiet in the classroom. Based on a survey of other audio analysis work, I decided that I would try to look at the timbre of the audio by focussing on patterns and structure in the frequency representation of the audio.

        <h3>Structure in Frequency Space</h3>

        A great way to visualize strucure in frequency space is by looking at an audio spectrogram. To generate a spectrogram, an audio signal is broken up into many short time bins, and then a fourier transform is calculated for each time bin to find its frequency spectrum. The frequency spectrum gives a weight for each possible frequency, and these weights are calculated for each time bin in the file. This information can be plotted by arranging all of the possible time bins and frequencies along the two axes of a plot, and using a colormap to label the frequency amplitudes at each point in the plot. This produces an image of the audio waveform, as shown below. Both of these images are lifted this <a href='https://www.youtube.com/watch?v=P9Kozlt0tTg'>youtube video</a> which is a fun watch if you have 3 minutes to spare.

        <div class='row'>
          <div class="col-sm-5 col-md-6">
            <img class="img-responsive" style="text-align:center" src="/static/Saxaphone.png" />
            <h4 style="text-align:center">Saxaphone spectrogram</h4>
          </div>
          <div class="col-sm-5 col-md-6">
            <img class="img-responsive" style="text-align:center" src="/static/Accordion.png" />
            <h4 style="text-align:center">Accordion spectrogram</h4>
          </div>
        </div>

        There is almost an overwhelming amount of data captured in the spectrogram. With large sets of labelled data, a good approach might be to train a convolutional neural net to look at the spectrogram like an image, capturing information about different frequencies that appear at the same time as well as the frequencies that appear close together in time. This kind of approach can be used to identify spoken words, or to <a href='https://yerevann.github.io/2015/10/11/spoken-language-identification-with-deep-convolutional-networks/'>identify the language spoken in audio</a>.

        Without a large set of labelled training data, I wouldn’t get very far in trying to train a neural network. Instead I wanted a more compact representation for the timbre information with which I could try to look for structure. The most common way to approach this in prior audio analysis work is to calculate the Mel Frequency Cepstral Coefficients.

        <h3>Why MFCCs?</h3>

        The Mel Frequency Cepstral Coefficients (hereafter MFCCs) are common features for audio analysis. For instance, used in <a href='https://github.com/embatbr/graduation-project/blob/master/docs/references/Comparative%20Evaluation%20of%20Various%20MFCC%20Implementations%20on%20the%20Speaker%20Verification%20Task.pdf'>speaker verification</a>, <a href='(http://www1.icsi.berkeley.edu/~knoxm/laughter_v10.pdf'>laughter detection</a>, and <a href='http://quod.lib.umich.edu/cgi/p/pod/dod-idx/use-of-mel-frequency-cepstral-coefficients-in-musical.pdf'>musical instrument identification</a>.

        They are the coefficients of the discrete cosine transform that operates on a modified frequency space representation of the signal. The modified frequency space is formed by collecting the frequency spectrum of a signal and applying a set of filterbanks that mimic the ranges of frequencies that humans can distinguish (using the Mel scale), and then taking the log of the frequency spectral amplitudes. Due to the way that the cosine transform is defined, the 0th order MFCC measures the total amplitude of the signal in the transformed frequency space. This means that the 0th order MFCC carries some information about the loudness of the sound.  The other MFCCs measure certain kinds of periodicity in the modified frequency space, which is what allows them to capture some sense of the harmonic structure of a sound and thus its timbre.

        To explore whether the various features for loudness, pitch, or timbre would actually allow us to separate different voices, I hand-labelled a classroom video to identify the seconds in which the teacher was talking. Then I plotted all of the time bins in the audio file in a two-dimensional audio feature space to explore whether the different voices appeared in distinct regions of the plot. Since there were many audio features to deal with, I used principal component analysis (PCA) to identify two composite audio features that I could plot in a flat image. 

        <div class='row'>
          <div class="col-5 col-md-5 col-centered">
            <img class="img-responsive center-block" src="/static/MFCC_features.png" />
            <h4 style="text-align:center">MFCC Features</h4>
          </div>
          <div class="col-5 col-md-5 col-centered">
            <img class="img-responsive center-block" src="/static/MFCC_and_Chroma.png" />
            <h4 style="text-align:center">MFCCs and Pitch Features</h4>
          </div>

        </div> 

        The plots above show these two-dimensional images derived from different types of audio features. In all cases the time bins in which the teacher or students are speaking are colored red and black, respectively. In an ideal world, the audio features for a teacher's vs. a student's voice  would be so different that they would occur in completely different regions of this two-dimensional image.  

        In the first image, only the MFCC features have been handed over to the PCA algorithm. In the resulting plot the red and black points are slightly separated, but do not form distinct and unrelated clusters that can be identified by eye. In the second plot, the same MFCC features have been combined with additional features that correspond to pitch instead of timbre. These "chroma" features are very useful for musical analysis since they map well onto the twelve tones of a standard musical scale. However, the two-dimensional projection returned by PCA shows an increased overlap of the red and black points - the opposite of the speaker separation that we would like to achieve.

        After trying to visualize many of the possible audio features in this way, I concluded that only the MFCC features seemed to be different for the two types of voices. The next step of the problem was to apply some unsupervised learning technique to try to detect the different speakers based on their MFCC features, without having any labelling to identify red or black points.

        <h3>Clustering the MFCC Features</h3>

        To create visualizations that can be easily viewed by a human, only two variables are desired. However, if the goal is to discover clusters of points in MFCC space, reducing to only two features is unnecessary. We could keep all twelve of the original MFCC features, or perhaps reduce to a smaller subset of more important features by again using PCA. In cases where I could run the model on a labelled video, I found that the performance was generally better if I reduced the feature space from twelve MFCCs to about five composite features. This reduction might help to avoid the "curse of dimensionality", or might just help to minimize the effect of certain MFCC features which are strongly correlated or which have little variation in the data.

        <div class='row'>
          <div class="col-6 col-md-6">
            <img class="img-responsive img-centred" src="/static/Unlabelled.png" />
            <h4 style="text-align:center">Unlabelled waveform</h4>
          </div>
        </div>
        <div class='row'>
          <div class="col-6 col-md-6">
            <img class="img-responsive img-centred" src="/static/Labelled.png" />
            <h4 style="text-align:center">Labelled waveform</h4>
          </div>
        </div> 

        Once I had a reduced set of features, I could calculate the distances between all of the time bins of the audio file (where distance is measured in this reduced features space) and then use hierarchical clustering to find the groups of time bins which are most similar to one another.  Depending on the assignment of each time bin into the two largest clusters, I could label each the entire audio waveform as “Speaker A” or “Speaker B”. This is shown above, where a cartoon audio waveform is color coded into black and red segments. It is important to note that due to the unsupervised nature of the model, I can’t actually say which speaker is the teacher! An extension to the project could try to find a way to determine which of many clusters corresponds to a teacher by training an additional classifier!

        <h3>Clustering the Seconds in a Time Series</h3>

        One idea that nagged at me while I was performing the clustering analysis was the complete ignorance of the temporal information. Since people in a conversation typically tend to speak for a few seconds at a time, any one second in the audio file is fairly likely to be spoken by the same person who spoke the previous few seconds. Looking at each second of audio as an independant point in “audio feature space” throws this useful information away. The problem isn’t just a theoretical one – the classification of the audio signal can jump back and forth between one speaker and the other erratically at some points in the audio file, in a way that clearly doesn’t correspond to the patterns of normal human speech.  I considered several approaches to try to bring the temporal information back into the model:

        <ol>
        <li>Use a few-second-long time bin to group the data together, and calculate only average audio features inside this large bin. This would weight several consecutive seconds together when generating the audio features. However, a student who speaks for a time that is short compared to the audio time bin would be ignored by the algorithm, since their distinct audio features would be overwhelmed by the several other seconds in the audio.</li>

        <li>Apply a median filter to the audio signal after the clustering analysis has been appied. This would force the label for each second to be more like its neighbors. I wasn’t excited about this approach, because a single time bin that really truly sounded very different from its neighboring time points would be treated exactly the same as a time point for which the audio features were right on the edge of being in one class or another. Since some student questions are short enough to exist in only one or two time bins in my analysis, median filtering might again overwhelm the student portions of the audio by the dominant teacher parts of the audio.</li>

        <li>Expand my feature set, so that for each time bin of audio I consider the MFCCs calculated on that time bin, but also the MFCCs calculated from the two time bins that came before and the two that come after. I reasoned that this would have the effect of making each time point look similar to its neighboring time points, and this similarity would be encoded before the time binned features get separated and fed into the clustering algorithm as independant measurements.</li>

        <li>I could define a new distance metric to use when judging which data points are “closest to one another” in the clustering algorithm. It’s not hard to define a distance metric that would make sense, but writing one in Python would slow down the clustering algorithm substantially (since the distance calculation is currently handled by scikit-learn, which has fast C code to do all of the heavy lifting).</li>

        </ol>

        I didn’t have enough time in the end to implement Option 4, so I used Option 3 to run all of my analysis. This performed a bit better than median filtering to remove the erratic jumps in the classification output over time, even though I don’t feel that it is as beautiful of an idea as a custom distance metric.

        <h3>Results</h3>

        There are some situations where the clusters output by the unsupervised algorithm correspond quite well to two different speakers. For example, <a href='http://www.grahamedge.com/video_output?yt_id=dqPjgQwoXLQ'>watch</a> what happens when the model is applied to an interview of Frank Sinatra by his daughter Nancy Sinatra. Although there are short periods in which the classification fails to capture a quick question by Nancy, overall the agreement is fairly good.

        I tweaked the hyperparameters of the unsupervised learning algorithm while monitoring thea accuracy with which it classified the seconds of a hand-labelled training video. I was rather pleased that after some parameter optimization, the model could identify 80% of the time bins in the video correctly. This lead to predictions of the "Teacher Talk Ratio" and "Number of Student Interactions" that seemed very reasonable compared to the true values I had measured by watching the video.

        <h3>Drawbacks</h3>

        However, when I pushed the model to generalize to a sample of other classroom videos, the performance was much worse. It seemed that there were certain edge cases, particular to each classroom video, that would throw the model for a loop. In some classes, a long period of silence would be detected as the most significant cluster in feature space, and all of the voices - both teacher and student - would be lumped together into the other cluster by the algorithm. In other cases, periods of music would be detected as a significantly different "voice" in the audio, and the model would again lump together the teacher and student voices in one large cluster.  These were indications that the method used was too general. I was separating the two most different parts of the audio, regardless of whether this was Teacher vs. Student voices, Voical Sounds vs. Music, or Noise vs. Silence.

        <!-- [click through to example of the spanish speaking teacher, where the silence clustering was very obvious] -->

        <!-- [interview between Rowan Atkinson and Elton John] -->

        To make the unsupervised approach very robust would require and identification of the various kinds of significant audio events that can occur in a classroom, followed by the development of safeguards to help the algorithm correct for each event. To keep the cluster analysis from latching on to a brief but distinct patch of audio, I added a check to the model output to detect strangely small clusters and re-merge them with the rest of the audio file. Further fixes of this sort could help to work around periods of silence in the file, but I had no further time to implement such fixes within the three-week time frame of Insight. 

        Trying to take the unsupervised clustering approach and make it more robust to classroom noise would be an exciting extension of this work. Alternatively, adding a small set of labelled data could help to improve the classification accuracy by attempting to train on the different audio structure of adults’ and childrens’ voices.

      </div>
    </div>
  </div>
</div>




        
  
    </div>

    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="static/js/bootstrap.min.js"></script>

    </div> <!-- /.container



    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
